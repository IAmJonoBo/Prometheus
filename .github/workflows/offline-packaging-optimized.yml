name: Offline Packaging
permissions:
  contents: read
  actions: write
"on":
  schedule:
    - cron: 0 3 * * 1 # Mondays at 03:00 UTC
  workflow_dispatch:
    inputs:
      force_full_rebuild:
        description: Force full rebuild of all platforms
        required: false
        default: false
        type: boolean
      platforms:
        description: "Platforms to build (comma-separated: ubuntu-latest,macos-latest,windows-latest)"
        required: false
        default: ubuntu-latest,macos-latest,windows-latest
        type: string
env:
  POETRY_HOME: ~/.poetry
  # Optimize LFS performance
  GIT_LFS_SKIP_SMUDGE: 1
  GIT_LFS_PROGRESS: 1
  # Parallel downloads for better performance
  PIP_RETRIES: 3
  PIP_TIMEOUT: 300
  PARALLEL_DOWNLOADS: 4
jobs:
  prepare-matrix:
    name: Prepare build matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
      cache-key: ${{ steps.cache-key.outputs.key }}
    steps:
      - name: Parse platform input
        id: matrix
        run: |
          platforms_input="${{ github.event.inputs.platforms }}"
          if [[ -n "${platforms_input}" ]]; then
            platforms="${platforms_input}"
          else
            platforms="ubuntu-latest,macos-latest,windows-latest"
          fi

          # Convert to JSON array
          json_platforms="["
          IFS=',' read -ra PLATFORMS <<<"${platforms}"
          for i in "${!PLATFORMS[@]}"; do
            if [ "${i}" -gt 0 ]; then
              json_platforms+=","
            fi
            json_platforms+="\"${PLATFORMS[$i]}\""
          done
          json_platforms+="]"

          matrix="{\"os\":$json_platforms}"
          echo "matrix=$matrix" >> "$GITHUB_OUTPUT"
          echo "Generated matrix: $matrix"
      - name: Generate cache key
        id: cache-key
        run: |
          # Include date and force rebuild flag in cache key
          date_key=$(date +%Y%m%d)
          force_key="${{ github.event.inputs.force_full_rebuild }}"
          key="v2-${date_key}-${force_key}"
          echo "key=$key" >> "$GITHUB_OUTPUT"
          echo "Cache key: $key"
  build-wheels:
    name: Build project wheels (${{ matrix.os }})
    needs: prepare-matrix
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.prepare-matrix.outputs.matrix) }}
    runs-on: ${{ matrix.os }}
    steps:
      - name: Configure Git for better LFS performance
        shell: bash
        run: |
          git config --global lfs.batch true
          git config --global lfs.transfertimeout 300
          git config --global lfs.activitytimeout 300
          git config --global lfs.dialtimeout 30
          git config --global lfs.concurrenttransfers 8
      - name: Ensure Git LFS tooling
        shell: bash
        run: |
          set -euo pipefail
          git lfs version
          git lfs install --local >/dev/null 2>&1 || git lfs install >/dev/null 2>&1
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ needs.prepare-matrix.outputs.cache-key }}-${{ hashFiles('**/pyproject.toml', '**/poetry.lock') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ needs.prepare-matrix.outputs.cache-key }}-
            ${{ runner.os }}-pip-
      - name: Reset workspace before checkout
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os
          import shutil
          import stat
          import subprocess
          from pathlib import Path

          root = Path(os.environ["GITHUB_WORKSPACE"])
          skip = {"_temp", ".cache"}
          git_dir = root / ".git"

          def handle_remove_readonly(func, path, exc):
              _, excvalue, _ = exc
              if isinstance(excvalue, PermissionError):
                  os.chmod(path, stat.S_IWRITE)
                  func(path)
              else:
                  raise excvalue

          if git_dir.exists():
              subprocess.run(
                  ["git", "reset", "--hard"],
                  cwd=root,
                  check=False,
                  stdout=subprocess.DEVNULL,
                  stderr=subprocess.DEVNULL,
              )
              subprocess.run(
                  ["git", "clean", "-ffdx"],
                  cwd=root,
                  check=False,
                  stdout=subprocess.DEVNULL,
                  stderr=subprocess.DEVNULL,
              )
          else:
              for child in root.iterdir():
                  if child.name in skip:
                      continue
                  if child.is_dir():
                      shutil.rmtree(child, onerror=handle_remove_readonly)
                  else:
                      try:
                          child.unlink()
                      except FileNotFoundError:
                          continue
              subprocess.run(["git", "init"], cwd=root, check=True)
          PY
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          clean: true
          fetch-depth: 0
          lfs: false
      - name: Optimized LFS content fetch
        shell: bash
        run: |
          set -euo pipefail

          # First check for missing objects
          if ! git remote get-url origin >/dev/null 2>&1; then
              echo "::warning::Skipping LFS remote verification because no origin remote is configured."
              exit 0
          fi

          # Use optimized LFS fetch with progress monitoring
          export GIT_LFS_PROGRESS=1

          # Fetch LFS objects with better timeout handling
          timeout 1200 git lfs fetch --all --verbose || {
              echo "::warning::LFS fetch timed out, attempting checkout of existing objects"
              git lfs checkout || echo "::warning::Some LFS objects may be missing"
          }

          # Verify and checkout
          git lfs checkout
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.12"
          cache: pip
      
      - name: Install cibuildwheel and build tools
        shell: bash
        run: |
          python -m pip install --upgrade pip
          pip install cibuildwheel build
      
      - name: Build platform-specific wheels with cibuildwheel
        shell: bash
        env:
          CIBW_BUILD: "cp311-* cp312-*"
          CIBW_SKIP: "*-musllinux_*"
          CIBW_ARCHS_LINUX: "x86_64 aarch64"
          CIBW_ARCHS_MACOS: "x86_64 arm64"
          CIBW_ARCHS_WINDOWS: "AMD64"
          CIBW_BUILD_VERBOSITY: 1
        run: |
          # Set platform-specific environment variables
          case "${{ runner.os }}" in
            Linux)
              export PLATFORM="linux_x86_64"
              ;;
            macOS)
              if [[ "$(uname -m)" == "arm64" ]]; then
                export PLATFORM="macosx_11_0_arm64"
              else
                export PLATFORM="macosx_10_9_x86_64"
              fi
              ;;
            Windows)
              export PLATFORM="win_amd64"
              ;;
          esac

          echo "Building wheels for platform: $PLATFORM with cibuildwheel"
          
          # Build wheels with cibuildwheel for multiple Python versions
          cibuildwheel --output-dir wheelhouse/platform/$PLATFORM
          
          # Also copy to main wheelhouse for compatibility
          mkdir -p wheelhouse
          cp wheelhouse/platform/$PLATFORM/*.whl wheelhouse/ 2>/dev/null || true

          # Create platform manifest with cibuildwheel metadata
          cat > "wheelhouse/platform/$PLATFORM/manifest.json" <<EOF
          {
              "platform": "$PLATFORM",
              "runner_os": "${{ runner.os }}",
              "runner_arch": "$(uname -m)",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "python_version": "$(python --version)",
              "build_tool": "cibuildwheel",
              "cibw_build": "$CIBW_BUILD",
              "wheels": $(find "wheelhouse/platform/$PLATFORM" -name "*.whl" -exec basename {} \; | jq -R . | jq -s .)
          }
          EOF
          
          echo "Built $(find "wheelhouse/platform/$PLATFORM" -name "*.whl" | wc -l) wheels for $PLATFORM"
      - name: Upload platform wheels
        uses: actions/upload-artifact@v5
        with:
          name: wheels-${{ runner.os }}-${{ runner.arch }}
          path: |
            wheelhouse/platform/**/*.whl
            wheelhouse/platform/**/manifest.json
          retention-days: 7
  dependency-suite:
    name: Dependency audit & multi-platform wheelhouse
    needs: [prepare-matrix, build-wheels]
    runs-on: ubuntu-latest
    env:
      EXTRAS: pii,observability,rag,llm,governance,integrations
      POETRY_NO_INTERACTION: "1"
    steps:
      - name: Configure Git for better LFS performance
        shell: bash
        run: |
          git config --global lfs.batch true
          git config --global lfs.transfertimeout 300
          git config --global lfs.activitytimeout 300
          git config --global lfs.dialtimeout 30
          git config --global lfs.concurrenttransfers 8
      - name: Ensure Git LFS tooling
        shell: bash
        run: |
          set -euo pipefail
          git lfs version
          git lfs install --local >/dev/null 2>&1 || git lfs install >/dev/null 2>&1
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ubuntu-pip-${{ needs.prepare-matrix.outputs.cache-key }}-${{ hashFiles('**/pyproject.toml', '**/poetry.lock') }}
          restore-keys: |
            ubuntu-pip-${{ needs.prepare-matrix.outputs.cache-key }}-
            ubuntu-pip-
      - name: Cache Poetry dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pypoetry
          key: ubuntu-poetry-${{ needs.prepare-matrix.outputs.cache-key }}-${{ hashFiles('**/poetry.lock') }}
          restore-keys: |
            ubuntu-poetry-${{ needs.prepare-matrix.outputs.cache-key }}-
            ubuntu-poetry-
      - name: Reset workspace before checkout
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os
          import shutil
          import stat
          import subprocess
          from pathlib import Path

          root = Path(os.environ["GITHUB_WORKSPACE"])
          skip = {"_temp", ".cache"}
          git_dir = root / ".git"

          def handle_remove_readonly(func, path, exc):
              _, excvalue, _ = exc
              if isinstance(excvalue, PermissionError):
                  os.chmod(path, stat.S_IWRITE)
                  func(path)
              else:
                  raise excvalue

          if git_dir.exists():
              subprocess.run(
                  ["git", "reset", "--hard"],
                  cwd=root,
                  check=False,
                  stdout=subprocess.DEVNULL,
                  stderr=subprocess.DEVNULL,
              )
              subprocess.run(
                  ["git", "clean", "-ffdx"],
                  cwd=root,
                  check=False,
                  stdout=subprocess.DEVNULL,
                  stderr=subprocess.DEVNULL,
              )
          else:
              for child in root.iterdir():
                  if child.name in skip:
                      continue
                  if child.is_dir():
                      shutil.rmtree(child, onerror=handle_remove_readonly)
                  else:
                      try:
                          child.unlink()
                      except FileNotFoundError:
                          continue
              subprocess.run(["git", "init"], cwd=root, check=True)
          PY
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          clean: true
          fetch-depth: 0
          lfs: false
      - name: Optimized LFS content fetch
        shell: bash
        run: |
          set -euo pipefail

          if ! git remote get-url origin >/dev/null 2>&1; then
              echo "::warning::Skipping LFS remote verification because no origin remote is configured."
              exit 0
          fi

          export GIT_LFS_PROGRESS=1
          timeout 1200 git lfs fetch --all --verbose || {
              echo "::warning::LFS fetch timed out, attempting checkout of existing objects"
              git lfs checkout || echo "::warning::Some LFS objects may be missing"
          }
          git lfs checkout
      - name: Setup Python and Poetry
        uses: ./.github/actions/setup-python-poetry
        with:
          python-version: "3.12"
          cache: pip
      - name: Install Poetry with caching
        run: |
          python -m pip install --upgrade pip
          pip install poetry==2.2.1 poetry-plugin-export
      - name: Validate pyproject metadata
        run: |
          poetry --version
          poetry lock --check
          poetry check
      - name: Install project dependencies with caching
        run: |
          poetry config cache-dir ~/.cache/pypoetry
          if [ -n "${EXTRAS:-}" ]; then
            poetry install --with "${EXTRAS}" --no-root --sync
          else
            poetry install --no-root --sync
          fi
      - name: Generate dependency report
        run: |
          poetry show --outdated --format json > dependency-report.json
      
      - name: Run dependency preflight and guard
        run: |
          # Run preflight to check dependency health
          mkdir -p var/dependency-preflight var/upgrade-guard
          poetry run prometheus deps preflight --json | tee var/dependency-preflight/latest.json
          
          # Run upgrade guard to assess risks
          poetry run prometheus deps guard \
            --preflight var/dependency-preflight/latest.json \
            --output var/upgrade-guard/assessment.json \
            --markdown var/upgrade-guard/summary.md \
            --snapshot-root var/upgrade-guard/runs \
            --snapshot-tag "offline-packaging" \
            --verbose || {
            echo "::warning::Upgrade guard completed with warnings"
          }
      
      - name: Run offline packaging orchestrator
        run: |
          # Use the prometheus offline-package command with full options
          poetry run prometheus offline-package \
            --output-dir vendor/offline \
            --auto-update \
            --auto-update-max minor || {
            echo "::warning::Offline package command had warnings, falling back to script"
            poetry run python scripts/offline_package.py --skip-phase git
          }
      
      - name: Validate offline package with doctor and remediation
        run: |
          # Run the offline doctor validation
          poetry run prometheus offline-doctor \
            --package-dir vendor/offline \
            --format json > var/offline-doctor-results.json || {
            echo "::warning::Offline package validation had warnings"
          }
          
          # Display results in table format for GitHub summary
          poetry run prometheus offline-doctor \
            --package-dir vendor/offline \
            --format table | tee -a "$GITHUB_STEP_SUMMARY"
          
          # Run remediation analysis on any failures
          if [ -f var/offline-doctor-results.json ]; then
            poetry run prometheus remediation wheelhouse \
              --input var/offline-doctor-results.json \
              --output var/remediation-recommendations.json || true
          fi
      - name: Download all platform wheels
        uses: actions/download-artifact@v6
        with:
          pattern: wheels-*
          path: vendor/wheelhouse/platform
          merge-multiple: false
      - name: Organize and verify platform wheels
        shell: bash
        run: |
          set -euo pipefail

          # Reorganize platform wheels for better structure
          find vendor/wheelhouse/platform -name "*.whl" -type f | while read -r wheel; do
              platform_dir=$(dirname "$wheel")
              platform_name=$(basename "$platform_dir")

              # Ensure platform directory exists in main wheelhouse
              mkdir -p "vendor/wheelhouse/platforms/$platform_name"

              # Copy wheel to platform-specific directory
              cp "$wheel" "vendor/wheelhouse/platforms/$platform_name/"

              # Also copy to main wheelhouse for compatibility
              cp "$wheel" "vendor/wheelhouse/"
          done

          # Create comprehensive manifest
          cat > vendor/wheelhouse/multi_platform_manifest.json <<EOF
          {
              "generated_at": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "platforms": $(find vendor/wheelhouse/platforms -name "manifest.json" -exec cat {} \; | jq -s .),
              "total_wheels": $(find vendor/wheelhouse -name "*.whl" | wc -l),
              "platform_count": $(find vendor/wheelhouse/platforms -maxdepth 1 -type d | wc -l),
              "extras_included": "${EXTRAS}"
          }
          EOF

          if ! find vendor/wheelhouse -type f -name "*.whl" -print -quit >/dev/null 2>&1; then
              echo "::error::No wheels were generated or downloaded."
              exit 1
          fi

          printf 'Multi-platform wheelhouse summary:\n'
          find vendor/wheelhouse/platforms -name "*.whl" | sort | sed 's/^/  - /'
      - name: Create optimized archives
        run: |
          # Create platform-specific archives
          cd vendor
          for platform_dir in wheelhouse/platforms/*/; do
              if [[ -d "$platform_dir" ]]; then
                  platform_name=$(basename "$platform_dir")
                  tar -czf "wheelhouse-${platform_name}.tar.gz" -C wheelhouse/platforms "$platform_name"
                  sha256sum "wheelhouse-${platform_name}.tar.gz" > "wheelhouse-${platform_name}.tar.gz.sha256"
              fi
          done

          # Create comprehensive archive
          tar -czf wheelhouse-all-platforms.tar.gz wheelhouse
          sha256sum wheelhouse-all-platforms.tar.gz > wheelhouse-all-platforms.tar.gz.sha256

          # Create models archive if present
          if [ -d models ]; then
            tar -czf models.tar.gz models
            sha256sum models.tar.gz > models.tar.gz.sha256
          fi

          # Create container images archive if present
          if [ -d images ]; then
            tar -czf images.tar.gz images
            sha256sum images.tar.gz > images.tar.gz.sha256
          fi

          # Create air-gapped deployment bundle
          mkdir -p airgapped-bundle

          # Copy deployment artifacts
          cp wheelhouse-all-platforms.tar.gz* airgapped-bundle/
          if [ -f models.tar.gz ]; then
            cp models.tar.gz* airgapped-bundle/
          fi
          if [ -f images.tar.gz ]; then
            cp images.tar.gz* airgapped-bundle/
          fi

          # Create deployment script for air-gapped environments
          cat > airgapped-bundle/deploy.sh << 'EOF'
          #!/bin/bash
          set -euo pipefail

          echo "ðŸš€ Prometheus Air-Gapped Deployment"
          echo "======================================"

          # Extract wheelhouse
          echo "ðŸ“¦ Extracting Python packages..."
          if [ -f wheelhouse-all-platforms.tar.gz ]; then
            tar -xzf wheelhouse-all-platforms.tar.gz
            echo "âœ… Python packages extracted"
          fi

          # Extract models if present
          if [ -f models.tar.gz ]; then
            echo "ðŸ§  Extracting AI models..."
            tar -xzf models.tar.gz
            echo "âœ… AI models extracted"
          fi

          # Extract container images if present
          if [ -f images.tar.gz ]; then
            echo "ðŸ³ Extracting container images..."
            tar -xzf images.tar.gz
            echo "âœ… Container images extracted"
          fi

          # Verify checksums
          echo "ðŸ” Verifying package integrity..."
          for checksum_file in *.sha256; do
            if [ -f "$checksum_file" ]; then
              sha256sum -c "$checksum_file" || {
                echo "âŒ Checksum verification failed for $checksum_file"
                exit 1
              }
            fi
          done
          echo "âœ… All checksums verified"

          # Install packages in offline mode
          echo "âš¡ Installing Python packages..."
          python -m pip install --no-index --find-links wheelhouse \
            -r wheelhouse/requirements.txt --force-reinstall || {
            echo "âš ï¸  Using fallback installation method..."
            python -m pip install --no-index --find-links wheelhouse prometheus-os
          }

          # Run validation
          echo "ðŸ”§ Validating installation..."
          python scripts/offline_doctor.py --package-dir . || {
            echo "âš ï¸  Validation warnings detected, but continuing..."
          }

          echo "ðŸŽ‰ Air-gapped deployment completed successfully!"
          echo "ðŸ“‹ Next steps:"
          echo "   1. Configure your environment variables"
          echo "   2. Run: poetry run prometheus --help"
          echo "   3. Start the pipeline: poetry run prometheus pipeline start"
          EOF

          chmod +x airgapped-bundle/deploy.sh

          # Create comprehensive bundle archive
          tar -czf airgapped-deployment-bundle.tar.gz airgapped-bundle
          sha256sum airgapped-deployment-bundle.tar.gz > airgapped-deployment-bundle.tar.gz.sha256

          echo "ðŸ“¦ Air-gapped deployment bundle created: airgapped-deployment-bundle.tar.gz"
      - name: Generate comprehensive summary
        run: |
          python - <<'PY'
          import json
          import os
          from pathlib import Path

          def safe_file_size(path):
              try:
                  return path.stat().st_size
              except:
                  return 0

          summary = {
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "summary": {}
          }

          # Platform wheel summary
          platforms_dir = Path('vendor/wheelhouse/platforms')
          if platforms_dir.exists():
              for platform_path in platforms_dir.iterdir():
                  if platform_path.is_dir():
                      wheel_count = len(list(platform_path.glob('*.whl')))
                      total_size = sum(safe_file_size(f) for f in platform_path.glob('*.whl'))
                      summary["summary"][platform_path.name] = {
                          "wheel_count": wheel_count,
                          "total_size_mb": round(total_size / (1024 * 1024), 2)
                      }

          # Archive summary
          vendor_dir = Path('vendor')
          archives = []
          for archive in vendor_dir.glob('*.tar.gz'):
              archives.append({
                  "name": archive.name,
                  "size_mb": round(safe_file_size(archive) / (1024 * 1024), 2)
              })
          summary["archives"] = archives

          # Dependency report summary
          dep_report_path = Path('dependency-report.json')
          if dep_report_path.exists():
              try:
                  dep_data = json.loads(dep_report_path.read_text())
                  summary["dependency_updates"] = len(dep_data.get('dependencies', []))
              except:
                  summary["dependency_updates"] = "unknown"
          
          # Upgrade guard assessment
          guard_path = Path('var/upgrade-guard/assessment.json')
          if guard_path.exists():
              try:
                  guard_data = json.loads(guard_path.read_text())
                  summary["guard_status"] = guard_data.get("status", "unknown")
                  summary["guard_risks"] = guard_data.get("risk_count", 0)
              except:
                  summary["guard_status"] = "unknown"
          
          # Remediation recommendations
          remediation_path = Path('var/remediation-recommendations.json')
          if remediation_path.exists():
              try:
                  remediation_data = json.loads(remediation_path.read_text())
                  summary["remediation_count"] = len(remediation_data.get("recommendations", []))
              except:
                  summary["remediation_count"] = 0

          # Build comprehensive GitHub summary
          step_summary = Path(os.environ['GITHUB_STEP_SUMMARY'])
          with step_summary.open('a') as f:
              f.write(f"""
## Multi-Platform Wheelhouse Build Summary

### Platform Wheels Generated (via cibuildwheel)
""")
              for platform, info in summary['summary'].items():
                  f.write(f"- **{platform}**: {info['wheel_count']} wheels ({info['total_size_mb']} MB)\n")
              
              f.write(f"""
### Archives Created
""")
              for archive in summary['archives']:
                  f.write(f"- **{archive['name']}**: {archive['size_mb']} MB\n")
              
              f.write(f"""
### Dependency Health
- **Updates Available**: {summary.get('dependency_updates', 'unknown')}
- **Guard Status**: {summary.get('guard_status', 'not run')}
- **Risk Count**: {summary.get('guard_risks', 0)}
- **Remediation Actions**: {summary.get('remediation_count', 0)}

### Integration with Local CLI

To sync these wheels locally and manage dependencies:

```bash
# Download the offline-packaging-suite-optimized artifact

# Sync with local deployment
poetry run prometheus deps sync --apply --force

# View dependency status
poetry run prometheus deps status

# Run upgrade planning
poetry run prometheus deps upgrade --sbom var/dependency-sync/sbom.json

# Validate offline package
poetry run prometheus offline-doctor --package-dir vendor/offline
```

This build supports air-gapped deployment across multiple platforms with optimized LFS storage and cibuildwheel multi-version support.
""")
          PY
      - name: Upload comprehensive artifacts
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: offline-packaging-suite-optimized
          if-no-files-found: warn
          retention-days: 30
          path: |
            dependency-report.json
            vendor/wheelhouse/**
            vendor/models/manifest.json
            vendor/images/manifest.json
            vendor/packaging-run.json
            vendor/CHECKSUMS.sha256
            vendor/*.tar.gz
            vendor/*.tar.gz.sha256
            vendor/airgapped-deployment-bundle.tar.gz*
            var/dependency-preflight/latest.json
            var/upgrade-guard/assessment.json
            var/upgrade-guard/summary.md
            var/upgrade-guard/runs/**
            var/offline-doctor-results.json
            var/remediation-recommendations.json
      - name: Clean up old artifacts
        if: always() && github.event_name != 'pull_request'
        uses: actions/github-script@v8
        with:
          github-token: ${{ github.token }}
          script: |-
            const artifactNames = ["offline-packaging-suite-optimized", "offline-packaging-suite"];
            const maxArtifacts = 5;

            for (const artifactName of artifactNames) {
              const artifacts = await github.rest.actions.listArtifactsForRepo({
                owner: context.repo.owner,
                repo: context.repo.repo,
                per_page: 100,
              });

              const candidates = artifacts.data.artifacts
                .filter((artifact) => artifact.name === artifactName && !artifact.expired)
                .sort((a, b) => new Date(b.created_at) - new Date(a.created_at));

              const toDelete = candidates.slice(maxArtifacts);

              if (toDelete.length === 0) {
                core.info(`No ${artifactName} artifacts require pruning.`);
                continue;
              }

              for (const artifact of toDelete) {
                core.info(`Deleting outdated artifact ${artifact.id} (${artifact.created_at})`);
                try {
                  await github.rest.actions.deleteArtifact({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    artifact_id: artifact.id,
                  });
                } catch (error) {
                  core.warning(`Failed to delete artifact ${artifact.id}: ${error?.message ?? error}`);
                }
              }
            }
