name: CI

# Streamlined CI pipeline for build, test, and containerization
# Optimized for GitHub.com with fallback support for GHES environments

on:
  push:
    branches: [main]
    paths-ignore:
      - "**.md"
      - docs/**
      - .gitignore
      - LICENSE
  pull_request:
    branches: [main]
    paths-ignore:
      - "**.md"
      - docs/**
      - .gitignore
      - LICENSE
  workflow_dispatch:

permissions:
  contents: read
  packages: write

env:
  # Override these via repository secrets/variables if needed
  RETENTION_DAYS: 30
  IMAGE_NAME: app
  # Skip LFS smudge for faster checkout; fetch explicitly if needed
  GIT_LFS_SKIP_SMUDGE: "1"

jobs:
  workflow-lint:
    name: Workflow lint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          lfs: false

      - name: Install actionlint and shellcheck
        run: |
          sudo apt-get update
          sudo apt-get install -y shellcheck
          curl -sSLo actionlint.tar.gz https://github.com/rhysd/actionlint/releases/download/v1.7.7/actionlint_1.7.7_linux_amd64.tar.gz
          tar -xf actionlint.tar.gz actionlint
          rm actionlint.tar.gz
          ./actionlint --version

      - name: Run actionlint
        run: |
          ./actionlint -color -shellcheck

  build:
    name: Build and package
    needs: workflow-lint
    runs-on: ubuntu-latest
    outputs:
      registry: ${{ steps.env-detect.outputs.registry }}
      image-tag: ${{ steps.env-detect.outputs.image-tag }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          clean: true
          fetch-depth: 0
          lfs: false

      - name: Check for macOS metadata artefacts
        run: bash scripts/check-macos-cruft.sh

      # Detect GitHub.com vs GHES environment
      - name: Detect environment
        id: env-detect
        shell: bash
        run: |
          SERVER_URL="${GITHUB_SERVER_URL:-https://github.com}"
          echo "server-url=${SERVER_URL}" >> "$GITHUB_OUTPUT"

          # Determine registry endpoint based on server URL
          if [[ "${SERVER_URL}" == "https://github.com" ]]; then
            REGISTRY="ghcr.io"
          else
            # Extract hostname from GHES URL and derive container registry
            HOSTNAME=$(echo "${SERVER_URL}" | sed -E 's#https?://##; s#/.*##')
            REGISTRY="containers.${HOSTNAME}"
          fi
          echo "registry=${REGISTRY}" >> "$GITHUB_OUTPUT"

          # Derive image tag from commit SHA (convert repository to lowercase)
          REPO_LOWER=$(echo "${{ github.repository }}" | tr '[:upper:]' '[:lower:]')
          IMAGE_TAG="${REGISTRY}/${REPO_LOWER}/${IMAGE_NAME}:${{ github.sha }}"
          echo "image-tag=${IMAGE_TAG}" >> "$GITHUB_OUTPUT"

          echo "Detected registry: ${REGISTRY}"
          echo "Image tag: ${IMAGE_TAG}"

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.12"
          # Disable cache by default; air-gapped environments handle it via vendored deps
          cache: ""

      # Language-aware caching: only enable if explicitly requested
      - name: Cache pip dependencies
        id: pip-cache
        uses: actions/cache@v4
        continue-on-error: true
        with:
          path: |
            ~/.cache/pip
            ~/.local/share/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/poetry.lock', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install build dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build wheel poetry==2.2.0 poetry-plugin-export || {
            echo "::error::Failed to install build dependencies"
            exit 1
          }

          echo "$HOME/.local/bin" >> "$GITHUB_PATH"
          export PATH="$HOME/.local/bin:$PATH"

          # Verify installations
          python -m pip --version
          poetry --version || echo "::warning::Poetry not in PATH, trying python -m poetry"
          poetry-plugin-export --version 2>/dev/null || python -m poetry self show plugins | grep export || echo "::warning::poetry-plugin-export may not be installed"

      # Use Poetry for dependency management (this is a Poetry project)
      - name: Install project dependencies
        continue-on-error: true
        run: |
          if [ -f poetry.lock ]; then
            echo "Installing dependencies with Poetry..."
            poetry install --no-root --only main || {
              echo "::warning::Poetry install failed; continuing with build-only deps"
            }
          else
            echo "::warning::poetry.lock not found, skipping dependency installation"
          fi

      - name: Validate dependency artefacts
        run: |
          bash scripts/manage-deps.sh --check || {
            echo "::warning::Dependency validation had issues"
          }

      # Build deliverables to ./dist
      - name: Build project
        run: |
          mkdir -p dist
          # Use Poetry to build if available, fallback to python -m build
          if command -v poetry >/dev/null 2>&1 && [ -f poetry.lock ]; then
            poetry build --format wheel --output dist || python -m build --outdir dist
          else
            python -m build --outdir dist || echo "::warning::Build step incomplete"
          fi
          # Add build metadata
          echo "Build timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)" > dist/BUILD_INFO
          echo "Git SHA: ${GITHUB_SHA:-unknown}" >> dist/BUILD_INFO

      # Build wheelhouse for offline installs
      - name: Build wheelhouse
        run: |
          echo "Building wheelhouse for offline deployment..."

          # Ensure Poetry is available even if console script is not on PATH
          if command -v poetry >/dev/null 2>&1; then
            export POETRY="poetry"
          else
            echo "::warning::Poetry CLI not found on PATH; using python -m poetry"
            export POETRY="python -m poetry"
          fi

          # Health check: Verify disk space
          available_space=$(df -BG . | awk 'NR==2 {print $4}' | sed 's/G//')
          if [ "${available_space}" -lt 5 ]; then
            echo "::warning::Low disk space: ${available_space}GB available"
          fi

          # Build wheelhouse with all extras
          EXTRAS="pii,observability,rag,llm,governance,integrations" \
          INCLUDE_DEV=true \
          CREATE_ARCHIVE=true \
          bash scripts/build-wheelhouse.sh dist/wheelhouse || {
            echo "::error::Wheelhouse build failed"
            exit 1
          }

          # Also add pip-audit for security scanning offline
          python -m pip download --dest dist/wheelhouse pip-audit || {
            echo "::warning::Failed to download pip-audit"
          }

          # Create requirements file for offline install
          if [ -f dist/wheelhouse/requirements.txt ]; then
            echo "pip-audit" >> dist/wheelhouse/requirements.txt
          fi

          # Generate manifest
          python - <<'PY'
          import json
          from datetime import datetime, timezone
          from pathlib import Path

          wheelhouse = Path("dist/wheelhouse")
          wheels = list(wheelhouse.glob("*.whl"))

          manifest = {
              "generated_at": datetime.now(timezone.utc).isoformat(),
              "commit": "${{ github.sha }}",
              "wheel_count": len(wheels),
              "wheels": [w.name for w in wheels],
              "extras": [
                  "pii",
                  "observability",
                  "rag",
                  "llm",
                  "governance",
                  "integrations",
              ],
              "include_dev": True,
              "create_archive": True,
              "includes_pip_audit": any("pip_audit" in w.name for w in wheels),
              "allow_sdist_used": [],
          }

          platform_manifest = wheelhouse / "platform_manifest.json"
          if platform_manifest.exists():
              try:
                  platform_data = json.loads(platform_manifest.read_text())
              except json.JSONDecodeError as exc:  # pragma: no cover - CI only
                  print(f"::warning::Failed to parse platform manifest: {exc}")
              else:
                  manifest["platform"] = platform_data.get("platform")
                  manifest["allow_sdist_for"] = platform_data.get(
                      "allow_sdist_for", []
                  )
                  manifest["allow_sdist_used"] = platform_data.get(
                      "allow_sdist_used", []
                  )
                  manifest["platform_metadata"] = platform_data

          (wheelhouse / "manifest.json").write_text(
              json.dumps(manifest, indent=2, sort_keys=True) + "\n"
          )
          print(f"Generated manifest with {len(wheels)} wheels")
          PY

      - name: Enforce binary-only wheelhouse
        run: |
          python - <<'PY'
          import json
          import sys
          from pathlib import Path

          manifest_path = Path("dist/wheelhouse/platform_manifest.json")
          if not manifest_path.exists():
              print("::warning::platform_manifest.json missing; skipping binary enforcement")
              sys.exit(0)

          try:
              data = json.loads(manifest_path.read_text())
          except json.JSONDecodeError as exc:  # pragma: no cover - CI only
              print(f"::error::Failed to parse platform manifest: {exc}")
              sys.exit(1)

          used = [item for item in data.get("allow_sdist_used", []) if item]
          if used:
              joined = ", ".join(sorted(set(used)))
              print(f"::error::Wheelhouse requires source builds for: {joined}")
              sys.exit(1)

          print("Wheelhouse contains binary wheels only.")
          PY

      # Validate offline package with doctor script
      - name: Validate offline package
        run: |
          if [ -d dist/wheelhouse ]; then
            echo "=== Running offline doctor validation ==="
            poetry run python scripts/offline_doctor.py --format table || {
              echo "::warning::Offline package validation had warnings"
            }
            
            echo ""
            echo "=== Running artifact verification ==="
            bash scripts/verify_artifacts.sh dist || {
              echo "::error::Artifact verification failed!"
              exit 1
            }
          else
            echo "::error::Wheelhouse directory not found at dist/wheelhouse"
            exit 1
          fi

      - name: Verify dist contents and create summary
        run: |
          echo "Contents of dist/:"
          if [ -d dist ]; then
            ls -lh dist/ || echo "dist/ is empty"
          else
            echo "dist/ directory does not exist yet"
          fi

          # Create workflow summary
          {
            printf "## Build Artifacts Summary\n\n"
            printf "### Package Build\n"
            printf "- **Git SHA**: \`%s\`\n" "${GITHUB_SHA:-unknown}"
            printf "- **Build Time**: \`%s\`\n\n" "$(date -u +"%Y-%m-%d %H:%M:%S UTC")"
          } >> "$GITHUB_STEP_SUMMARY"

          if [ -d dist/wheelhouse ]; then
            wheel_count=$(find dist/wheelhouse -type f -name "*.whl" 2>/dev/null | wc -l | tr -d ' ')
            wheelhouse_size=$(du -sh dist/wheelhouse 2>/dev/null | cut -f1)
            {
              printf "### Wheelhouse\n"
              printf "- **Wheel Count**: %s\n" "${wheel_count:-0}"
              printf "- **Total Size**: %s\n" "${wheelhouse_size:-0}"
              printf "- **Location**: \`dist/wheelhouse/\`\n"
              if find dist/wheelhouse -maxdepth 1 -type f -name "pip_audit*.whl" 2>/dev/null | grep -q .; then
                printf "- **Includes pip-audit**: ✅ Yes\n"
              else
                printf "- **Includes pip-audit**: ⚠️ No\n"
              fi
              printf "\n"
            } >> "$GITHUB_STEP_SUMMARY"

            if [ -f dist/wheelhouse/manifest.json ]; then
              python - <<'PY' >> "$GITHUB_STEP_SUMMARY"
import json
from pathlib import Path
p=Path('dist/wheelhouse/manifest.json')
try:
    data=json.loads(p.read_text())
except Exception as e:
    print(f"- manifest.json parse error: {e}")
else:
    s=[item for item in data.get('allow_sdist_used', []) if item]
    print('- **Source build fallback**: ⚠️ ' + ', '.join(sorted(set(s))) if s else '- **Source build fallback**: ✅ None')
    # Wheel inventory (first 10)
    wheels=sorted([w.name for w in Path('dist/wheelhouse').glob('*.whl')])
    if wheels:
        print('- **Wheel inventory**:')
        for w in wheels[:10]:
            print(f'  - {w}')
        if len(wheels) > 10:
            print(f'  - … and {len(wheels)-10} more')
PY
              printf "\n" >> "$GITHUB_STEP_SUMMARY"
            fi
          else
            {
              printf "### Wheelhouse\n"
              printf "- **Status**: ⚠️ Not found at \`dist/wheelhouse\`\n\n"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

          if compgen -G "dist/*.whl" > /dev/null; then
            {
              printf "### Python Wheel\n"
              printf "- Built successfully ✅\n\n"
            } >> "$GITHUB_STEP_SUMMARY"
          else
            {
              printf "### Python Wheel\n"
              printf "- **Status**: ⚠️ No wheel found in \`dist/\`\n\n"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

          {
            printf "### Dist Listing\n"
            printf "\n"
            printf "```text\n"
            ls -lh dist/ 2>/dev/null || echo "(dist missing)"
            printf "\n````\n"
          } >> "$GITHUB_STEP_SUMMARY"

          printf "All artifacts will be uploaded as \`app_bundle\` with %s-day retention.\n" "${RETENTION_DAYS:-30}" >> "$GITHUB_STEP_SUMMARY"

      # Upload artifact with actions/upload-artifact@v4
      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: app_bundle
          path: dist/
          retention-days: ${{ env.RETENTION_DAYS }}
          if-no-files-found: warn

  publish:
    name: Publish container image
    needs: build
    runs-on: ubuntu-latest
    # Only run if Docker is available; gracefully skip otherwise
    if: ${{ always() }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          clean: true
          fetch-depth: 1
          lfs: false

      - name: Download build artifacts
        uses: actions/download-artifact@v5
        with:
          name: app_bundle
          path: dist/

      - name: Check Docker availability
        id: docker-check
        shell: bash
        run: |
          if command -v docker >/dev/null 2>&1; then
            echo "available=true" >> "$GITHUB_OUTPUT"
            docker --version
          else
            echo "available=false" >> "$GITHUB_OUTPUT"
            echo "::warning::Docker not available; skipping container publish"
          fi

      # GHES branch: use native docker login if Marketplace actions unavailable
      - name: Log in to container registry (native)
        if: steps.docker-check.outputs.available == 'true'
        shell: bash
        env:
          REGISTRY: ${{ needs.build.outputs.registry }}
        run: |
          # Use helper script for offline-friendly docker login
          bash scripts/ci/docker-login.sh \
            "${REGISTRY}" \
            "${{ github.actor }}" \
            "${{ secrets.GITHUB_TOKEN }}"

      - name: Build container image
        if: steps.docker-check.outputs.available == 'true'
        shell: bash
        env:
          IMAGE_TAG: ${{ needs.build.outputs.image-tag }}
        run: |
          # Create minimal Dockerfile if not present
          if [ ! -f Dockerfile ]; then
            cat > Dockerfile <<'EOF'
          FROM python:3.12-slim
          WORKDIR /app
          COPY dist/ /app/dist/
          RUN echo "Container built at $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          CMD ["python", "--version"]
          EOF
          fi

          docker build -t "${IMAGE_TAG}" .
          echo "Built image: ${IMAGE_TAG}"

      - name: Push container image
        if: steps.docker-check.outputs.available == 'true'
        shell: bash
        env:
          IMAGE_TAG: ${{ needs.build.outputs.image-tag }}
        run: |
          docker push "${IMAGE_TAG}" || {
            echo "::warning::docker push failed; check registry permissions"
            exit 0
          }
          echo "Pushed image: ${IMAGE_TAG}"

  consume:
    name: Consumer job (restricted runners)
    needs: [build, publish]
    runs-on: ubuntu-latest
    if: ${{ always() }}
    steps:
      - name: Checkout repository (for scripts)
        uses: actions/checkout@v5
        with:
          sparse-checkout: |
            scripts/verify_artifacts.sh
          sparse-checkout-cone-mode: false

      - name: Download artifact
        uses: actions/download-artifact@v5
        with:
          name: app_bundle
          path: /tmp/payload

      - name: Install from artifact
        shell: bash
        run: |
          echo "=== Artifact Contents ==="
          ls -lh /tmp/payload/

          # Check for BUILD_INFO
          if [ -f /tmp/payload/BUILD_INFO ]; then
            echo ""
            echo "=== Build Info ==="
            cat /tmp/payload/BUILD_INFO
          fi

          # Run comprehensive artifact verification
          echo ""
          echo "=== Running Artifact Verification ==="
          bash scripts/verify_artifacts.sh /tmp/payload --test || {
            echo "::error::Artifact verification failed in consumer environment!"
            exit 1
          }

          echo ""
          echo "✅ Offline package validation successful in simulated air-gapped environment"

      - name: Demonstrate docker pull path
        if: needs.publish.result == 'success'
        shell: bash
        env:
          IMAGE_TAG: ${{ needs.build.outputs.image-tag }}
        run: |
          if command -v docker >/dev/null 2>&1; then
            echo "Attempting to pull ${IMAGE_TAG}"
            docker pull "${IMAGE_TAG}" || {
              echo "::notice::Image pull failed or not available"
            }
          else
            echo "::notice::Docker not available on consumer runner"
          fi

  # Cleanup old artifacts to manage storage
  cleanup:
    name: Cleanup old artifacts
    needs: [build, publish, consume]
    runs-on: ubuntu-latest
    if: ${{ always() && github.event_name != 'pull_request' }}
    permissions:
      actions: write
    steps:
      - name: Prune old CI artifacts
        uses: actions/github-script@v8
        continue-on-error: true
        with:
          script: |
            const maxArtifacts = 5;  // Keep last 5 builds
            const artifactName = "app_bundle";

            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 100,
            });

            const candidates = artifacts.data.artifacts
              .filter((artifact) => artifact.name === artifactName && !artifact.expired)
              .sort((a, b) => new Date(b.created_at) - new Date(a.created_at));

            const toDelete = candidates.slice(maxArtifacts);

            if (toDelete.length === 0) {
              core.info(`No ${artifactName} artifacts require pruning.`);
              return;
            }

            for (const artifact of toDelete) {
              core.info(`Deleting old artifact ${artifact.id} from ${artifact.created_at}`);
              try {
                await github.rest.actions.deleteArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id,
                });
              } catch (error) {
                core.warning(`Failed to delete artifact ${artifact.id}: ${error.message}`);
              }
            }
