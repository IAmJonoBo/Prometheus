name: CI
# Streamlined CI pipeline for build, test, and containerization
# Optimized for GitHub.com with fallback support for GHES environments
on:
  push:
    branches: [main]
    paths-ignore:
      - "**.md"
      - docs/**
      - .gitignore
      - LICENSE
  pull_request:
    branches: [main]
    paths-ignore:
      - "**.md"
      - docs/**
      - .gitignore
      - LICENSE
  workflow_dispatch:
permissions:
  contents: read
  packages: write
env:
  # Override these via repository secrets/variables if needed
  RETENTION_DAYS: 30
  IMAGE_NAME: app
  # Skip LFS smudge for faster checkout; fetch explicitly if needed
  GIT_LFS_SKIP_SMUDGE: "1"
jobs:
  workflow-lint:
    name: Workflow lint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          lfs: false
      - name: Determine lint runner
        id: lint-mode
        shell: bash
        env:
          TRUNK_SUPPRESS_TELEMETRY: "1"
        run: |
          if [ -x ./.trunk/tools/trunk ]; then
            echo "Trunk CLI detected; using vendored toolchain" >&2
            echo "mode=trunk" >> "$GITHUB_OUTPUT"
          else
            echo "Trunk CLI missing; falling back to portable linters" >&2
            echo "mode=fallback" >> "$GITHUB_OUTPUT"
            bash scripts/ci/install-actionlint.sh
          fi
      - name: Run Trunk lint (actionlint + shellcheck)
        if: steps.lint-mode.outputs.mode == 'trunk'
        env:
          TRUNK_SUPPRESS_TELEMETRY: "1"
        run: |
          ./.trunk/tools/trunk check --ci --filter=actionlint,shellcheck
      - name: Run actionlint fallback
        if: steps.lint-mode.outputs.mode == 'fallback'
        run: |
          if [ -z "${ACTIONLINT_BIN:-}" ]; then
            echo "::error::ACTIONLINT_BIN not set by install-actionlint.sh"
            exit 1
          fi
          echo "Using actionlint binary at ${ACTIONLINT_BIN}"
          "${ACTIONLINT_BIN}" -color
      - name: Run shellcheck fallback
        if: steps.lint-mode.outputs.mode == 'fallback'
        shell: bash
        run: |
          if ! command -v shellcheck >/dev/null 2>&1; then
            echo "::warning::shellcheck not available; skipping shell lint"
            exit 0
          fi

          mapfile -t files < <(git ls-files '*.sh')
          if [ "${#files[@]}" -eq 0 ]; then
            echo "No shell scripts detected; skipping shellcheck"
            exit 0
          fi

          shellcheck "${files[@]}"
  quality-gates:
    name: Type checking and tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          lfs: false
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.12"
          cache: pip
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install poetry==2.2.1
          poetry install --no-root --with dev
      - name: Run type checker (mypy)
        run: |
          echo "Running mypy type checker..."
          MYPYPATH=. poetry run mypy --config-file mypy.ini --show-error-codes --pretty \
            common/ ingestion/ retrieval/ reasoning/ decision/ execution/ monitoring/ \
            || {
            echo "::warning::Type checking found errors. These should be addressed."
            # Don't fail the build yet during migration period
            exit 0
          }
      - name: Run linter (ruff)
        run: |
          poetry run ruff check --output-format=github
      - name: Run tests with coverage
        run: |
          poetry run pytest --cov=. --cov-report=term-missing --cov-report=xml \
            --cov-fail-under=60 \
            -v
      - name: Upload coverage report
        uses: codecov/codecov-action@v5
        if: always()
        continue-on-error: true
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
  build:
    name: Build and package
    needs: [workflow-lint, quality-gates]
    runs-on: ubuntu-latest
    outputs:
      registry: ${{ steps.env-detect.outputs.registry }}
      image-tag: ${{ steps.env-detect.outputs.image-tag }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          clean: true
          fetch-depth: 0
          lfs: false
      - name: Check for macOS metadata artefacts
        run: bash scripts/check-macos-cruft.sh
      # Detect GitHub.com vs GHES environment
      - name: Detect environment
        id: env-detect
        shell: bash
        run: |
          SERVER_URL="${GITHUB_SERVER_URL:-https://github.com}"
          echo "server-url=${SERVER_URL}" >> "$GITHUB_OUTPUT"

          # Determine registry endpoint based on server URL
          if [[ "${SERVER_URL}" == "https://github.com" ]]; then
            REGISTRY="ghcr.io"
          else
            # Extract hostname from GHES URL and derive container registry
            HOSTNAME=$(echo "${SERVER_URL}" | sed -E 's#https?://##; s#/.*##')
            REGISTRY="containers.${HOSTNAME}"
          fi
          echo "registry=${REGISTRY}" >> "$GITHUB_OUTPUT"

          # Derive image tag from commit SHA (convert repository to lowercase)
          REPO_LOWER=$(echo "${{ github.repository }}" | tr '[:upper:]' '[:lower:]')
          IMAGE_TAG="${REGISTRY}/${REPO_LOWER}/${IMAGE_NAME}:${{ github.sha }}"
          echo "image-tag=${IMAGE_TAG}" >> "$GITHUB_OUTPUT"

          echo "Detected registry: ${REGISTRY}"
          echo "Image tag: ${IMAGE_TAG}"
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.12"
          # Disable cache by default; air-gapped environments handle it via vendored deps
          cache: ""
      # Language-aware caching: only enable if explicitly requested
      - name: Cache pip dependencies
        id: pip-cache
        uses: actions/cache@v4
        continue-on-error: true
        with:
          path: |
            ~/.cache/pip
            ~/.local/share/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/poetry.lock', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      - name: Install build dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build wheel poetry==2.2.1 poetry-plugin-export || {
            echo "::error::Failed to install build dependencies"
            exit 1
          }

          echo "$HOME/.local/bin" >> "$GITHUB_PATH"
          export PATH="$HOME/.local/bin:$PATH"

          # Verify installations
          python -m pip --version
          poetry --version || echo "::warning::Poetry not in PATH, trying python -m poetry"
          poetry-plugin-export --version 2>/dev/null \
            || python -m poetry self show plugins | grep export \
            || echo "::warning::poetry-plugin-export may not be installed"
      # Use Poetry for dependency management (this is a Poetry project)
      - name: Install project dependencies
        continue-on-error: true
        run: |
          if [ -f poetry.lock ]; then
            echo "Installing dependencies with Poetry..."
            poetry install --no-root --only main || {
              echo "::warning::Poetry install failed; continuing with build-only deps"
            }
          else
            echo "::warning::poetry.lock not found, skipping dependency installation"
          fi
      - name: Validate dependency artefacts
        run: |
          bash scripts/manage-deps.sh --check || {
            echo "::warning::Dependency validation had issues"
          }
      # Build deliverables to ./dist
      - name: Build project
        run: |
          mkdir -p dist
          # Use Poetry to build if available, fallback to python -m build
          if command -v poetry >/dev/null 2>&1 && [ -f poetry.lock ]; then
            poetry build --format wheel --output dist || python -m build --outdir dist
          else
            python -m build --outdir dist || echo "::warning::Build step incomplete"
          fi
          # Add build metadata
          echo "Build timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)" > dist/BUILD_INFO
          echo "Git SHA: ${GITHUB_SHA:-unknown}" >> dist/BUILD_INFO
      # Build wheelhouse for offline installs
      - name: Build wheelhouse
        run: |
          echo "Building wheelhouse for offline deployment..."

          # Ensure Poetry is available even if console script is not on PATH
          if command -v poetry >/dev/null 2>&1; then
            export POETRY="poetry"
          else
            echo "::warning::Poetry CLI not found on PATH; using python -m poetry"
            export POETRY="python -m poetry"
          fi

          # Health check: Verify disk space
          available_space=$(df -BG . | awk 'NR==2 {print $4}' | sed 's/G//')
          if [ "${available_space}" -lt 5 ]; then
            echo "::warning::Low disk space: ${available_space}GB available"
          fi

          # Build wheelhouse with all extras
          EXTRAS="pii,observability,rag,llm,governance,integrations" \
          INCLUDE_DEV=true \
          CREATE_ARCHIVE=true \
          bash scripts/build-wheelhouse.sh dist/wheelhouse || {
            echo "::error::Wheelhouse build failed"
            exit 1
          }

          # Also add pip-audit for security scanning offline
          python -m pip download --dest dist/wheelhouse pip-audit || {
            echo "::warning::Failed to download pip-audit"
          }

          # Create requirements file for offline install
          if [ -f dist/wheelhouse/requirements.txt ]; then
            echo "pip-audit" >> dist/wheelhouse/requirements.txt
          fi

          # Generate manifest
          python - <<'PY'
          import json
          from datetime import datetime, timezone
          from pathlib import Path

          wheelhouse = Path("dist/wheelhouse")
          wheels = list(wheelhouse.glob("*.whl"))

          manifest = {
              "generated_at": datetime.now(timezone.utc).isoformat(),
              "commit": "${{ github.sha }}",
              "wheel_count": len(wheels),
              "wheels": [w.name for w in wheels],
              "extras": [
                  "pii",
                  "observability",
                  "rag",
                  "llm",
                  "governance",
                  "integrations",
              ],
              "include_dev": True,
              "create_archive": True,
              "includes_pip_audit": any("pip_audit" in w.name for w in wheels),
              "allow_sdist_used": [],
          }

          platform_manifest = wheelhouse / "platform_manifest.json"
          if platform_manifest.exists():
              try:
                  platform_data = json.loads(platform_manifest.read_text())
              except json.JSONDecodeError as exc:  # pragma: no cover - CI only
                  print(f"::warning::Failed to parse platform manifest: {exc}")
              else:
                  manifest["platform"] = platform_data.get("platform")
                  manifest["allow_sdist_for"] = platform_data.get(
                      "allow_sdist_for", []
                  )
                  manifest["allow_sdist_used"] = platform_data.get(
                      "allow_sdist_used", []
                  )
                  manifest["platform_metadata"] = platform_data

          (wheelhouse / "manifest.json").write_text(
              json.dumps(manifest, indent=2, sort_keys=True) + "\n"
          )
          print(f"Generated manifest with {len(wheels)} wheels")
          PY
      - name: Enforce binary-only wheelhouse
        run: |
          python - <<'PY'
          import json
          import sys
          from pathlib import Path

          manifest_path = Path("dist/wheelhouse/platform_manifest.json")
          if not manifest_path.exists():
              print("::warning::platform_manifest.json missing; skipping binary enforcement")
              sys.exit(0)

          try:
              data = json.loads(manifest_path.read_text())
          except json.JSONDecodeError as exc:  # pragma: no cover - CI only
              print(f"::error::Failed to parse platform manifest: {exc}")
              sys.exit(1)

          used = [item for item in data.get("allow_sdist_used", []) if item]
          if used:
              joined = ", ".join(sorted(set(used)))
              print(f"::error::Wheelhouse requires source builds for: {joined}")
              sys.exit(1)

          print("Wheelhouse contains binary wheels only.")
          PY
      # Validate offline package with doctor script
      - name: Validate offline package
        run: |
          if [ -d dist/wheelhouse ]; then
            echo "=== Running offline doctor validation ==="
            poetry run python scripts/offline_doctor.py --format table || {
              echo "::warning::Offline package validation had warnings"
            }

            echo ""
            echo "=== Running artifact verification ==="
            bash scripts/verify_artifacts.sh dist || {
              echo "::error::Artifact verification failed!"
              exit 1
            }
          else
            echo "::error::Wheelhouse directory not found at dist/wheelhouse"
            exit 1
          fi
      - name: Verify dist contents and create summary
        run: |
          echo "Contents of dist/:"
          if [ -d dist ]; then
            ls -lh dist/ || echo "dist/ is empty"
          else
            echo "dist/ directory does not exist yet"
          fi

          if [ -f scripts/summarize_dist.sh ]; then
            bash scripts/summarize_dist.sh dist >> "$GITHUB_STEP_SUMMARY"
          else
            {
              printf "## Build Artifacts Summary\n\n"
              printf "### Package Build\n"
              printf "- **Git SHA**: \`%s\`\n" "${GITHUB_SHA:-unknown}"
              printf "- **Build Time**: \`%s\`\n\n" "$(date -u +"%Y-%m-%d %H:%M:%S UTC")"
              printf "_scripts/summarize_dist.sh missing; summary truncated_\n"
            } >> "$GITHUB_STEP_SUMMARY"
          fi
      # Upload artifact with actions/upload-artifact@v4
      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: app_bundle
          path: dist/
          retention-days: ${{ env.RETENTION_DAYS }}
          if-no-files-found: warn
  publish:
    name: Publish container image
    needs: build
    runs-on: ubuntu-latest
    # Only run after a successful build; skip cleanly if the build failed or was skipped
    if: ${{ needs.build.result == 'success' }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          clean: true
          fetch-depth: 1
          lfs: false
      - name: Download build artifacts
        uses: actions/download-artifact@v5
        with:
          name: app_bundle
          path: dist/
      - name: Check Docker availability
        id: docker-check
        shell: bash
        run: |
          if command -v docker >/dev/null 2>&1; then
            echo "available=true" >> "$GITHUB_OUTPUT"
            docker --version
          else
            echo "available=false" >> "$GITHUB_OUTPUT"
            echo "::warning::Docker not available; skipping container publish"
          fi
      # GHES branch: use native docker login if Marketplace actions unavailable
      - name: Log in to container registry (native)
        if: steps.docker-check.outputs.available == 'true'
        shell: bash
        env:
          REGISTRY: ${{ needs.build.outputs.registry }}
        run: |
          # Use helper script for offline-friendly docker login
          bash scripts/ci/docker-login.sh \
            "${REGISTRY}" \
            "${{ github.actor }}" \
            "${{ secrets.GITHUB_TOKEN }}"
      - name: Build container image
        if: steps.docker-check.outputs.available == 'true'
        shell: bash
        env:
          IMAGE_TAG: ${{ needs.build.outputs.image-tag }}
        run: |
          # Create minimal Dockerfile if not present
          if [ ! -f Dockerfile ]; then
            cat > Dockerfile <<'EOF'
          FROM python:3.12-slim
          WORKDIR /app
          COPY dist/ /app/dist/
          RUN echo "Container built at $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          CMD ["python", "--version"]
          EOF
          fi

          docker build -t "${IMAGE_TAG}" .
          echo "Built image: ${IMAGE_TAG}"
      - name: Push container image
        if: steps.docker-check.outputs.available == 'true'
        shell: bash
        env:
          IMAGE_TAG: ${{ needs.build.outputs.image-tag }}
        run: |
          docker push "${IMAGE_TAG}" || {
            echo "::warning::docker push failed; check registry permissions"
            exit 0
          }
          echo "Pushed image: ${IMAGE_TAG}"
  consume:
    name: Consumer job (restricted runners)
    needs: [build, publish]
    runs-on: ubuntu-latest
    # Require a successful build; allow publish to be skipped (e.g., no Docker)
    if: ${{ needs.build.result == 'success' && (needs.publish.result == 'success' || needs.publish.result == 'skipped') }}
    steps:
      - name: Checkout repository (for scripts)
        uses: actions/checkout@v5
        with:
          sparse-checkout: |
            scripts/verify_artifacts.sh
          sparse-checkout-cone-mode: false
      - name: Download artifact
        uses: actions/download-artifact@v5
        with:
          name: app_bundle
          path: /tmp/payload
      - name: Install from artifact
        shell: bash
        run: |
          echo "=== Artifact Contents ==="
          ls -lh /tmp/payload/

          # Check for BUILD_INFO
          if [ -f /tmp/payload/BUILD_INFO ]; then
            echo ""
            echo "=== Build Info ==="
            cat /tmp/payload/BUILD_INFO
          fi

          # Run comprehensive artifact verification
          echo ""
          echo "=== Running Artifact Verification ==="
          bash scripts/verify_artifacts.sh /tmp/payload --test || {
            echo "::error::Artifact verification failed in consumer environment!"
            exit 1
          }

          echo ""
          echo "✅ Offline package validation successful in simulated air-gapped environment"
      - name: Demonstrate docker pull path
        if: needs.publish.result == 'success'
        shell: bash
        env:
          IMAGE_TAG: ${{ needs.build.outputs.image-tag }}
        run: |
          if command -v docker >/dev/null 2>&1; then
            echo "Attempting to pull ${IMAGE_TAG}"
            docker pull "${IMAGE_TAG}" || {
              echo "::notice::Image pull failed or not available"
            }
          else
            echo "::notice::Docker not available on consumer runner"
          fi
  # Cleanup old artifacts to manage storage
  cleanup:
    name: Cleanup old artifacts
    needs: [build, publish, consume]
    runs-on: ubuntu-latest
    if: ${{ always() && github.event_name != 'pull_request' }}
    permissions:
      actions: write
    steps:
      - name: Prune old CI artifacts
        uses: actions/github-script@v8
        continue-on-error: true
        with:
          script: |-
            const maxArtifacts = 5;  // Keep last 5 builds
            const artifactName = "app_bundle";

            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 100,
            });

            const candidates = artifacts.data.artifacts
              .filter((artifact) => artifact.name === artifactName && !artifact.expired)
              .sort((a, b) => new Date(b.created_at) - new Date(a.created_at));

            const toDelete = candidates.slice(maxArtifacts);

            if (toDelete.length === 0) {
              core.info(`No ${artifactName} artifacts require pruning.`);
              return;
            }

            for (const artifact of toDelete) {
              core.info(`Deleting old artifact ${artifact.id} from ${artifact.created_at}`);
              try {
                await github.rest.actions.deleteArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id,
                });
              } catch (error) {
                core.warning(`Failed to delete artifact ${artifact.id}: ${error.message}`);
              }
            }
